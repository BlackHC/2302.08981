{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a1dd90",
   "metadata": {},
   "source": [
    "# Reproducing or downloading the benchmark results and running new configurations on the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22754a6c",
   "metadata": {},
   "source": [
    "## Reproducing or downloading our results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad09446",
   "metadata": {},
   "source": [
    "In this notebook, we discuss how to reproduce the results from our paper and how to benchmark your own methods. Before running this notebook, please follow the installation and data download instructions from the README.md file of the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6daea8",
   "metadata": {},
   "source": [
    "We will now change the working directory from the examples subfolder to the main folder, which is required for the imports to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a9b9aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1db3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')   # change directory inside the notebook to the main directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1158a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/blackhc/PycharmProjects/bmdal_reg\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41586197",
   "metadata": {},
   "source": [
    "## Running custom configurations on the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3626a822",
   "metadata": {},
   "source": [
    "If you want to run your own configurations on the benchmark, you may want to take a look at the code in `run_experiments.py`. Here, we will show a minimalistic example of how to run two custom benchmark configurations, which can run on a CPU in a few minutes. A few other files you may find helpful are:\n",
    "- `test_single_task.py` allows you to run a single BMDAL configuration on a single split of a single data set, for fast exploration.\n",
    "- `rename_algs.py` contains a few helper functions to modify/rename/remove saved results. It can be used for example if the names of some experiment results should be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70779de2",
   "metadata": {},
   "source": [
    "First, we need to create a list of configurations that will be executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4d04ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmdal_reg.run_experiments import RunConfigList\n",
    "from bmdal_reg.train import ModelTrainer\n",
    "from bmdal_reg.sklearn_models import RandomForestRegressor, CatBoostRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "# some general configuration for the NN and active learning\n",
    "kwargs = dict(post_sigma=1e-3, maxdet_sigma=1e-3, weight_gain=0.2, bias_gain=0.2, lr=0.375, act='relu')\n",
    "run_configs = RunConfigList()\n",
    "#run_configs.append(1e-6, ModelTrainer(f'NN_random', selection_method='random', create_model=RandomForestRegressor,\n",
    "                                   #base_kernel='linear', kernel_transforms=[], **kwargs))\n",
    "run_configs.append(4e-6, ModelTrainer(f'HGR_lcmd-tp_predictions', selection_method='lcmd', sel_with_train=True, create_model=HistGradientBoostingRegressor,\n",
    "                                             base_kernel='predictions', kernel_transforms=[], **kwargs))\n",
    "\n",
    "run_configs.append(4e-6, ModelTrainer(f'CAT_lcmd-tp_predictions', selection_method='lcmd', sel_with_train=True, create_model=CatBoostRegressor,\n",
    "                                             base_kernel='predictions', kernel_transforms=[], **kwargs))\n",
    "run_configs.append(4e-6, ModelTrainer(f'RF_lcmd-tp_predictions', selection_method='lcmd', sel_with_train=True, create_model=RandomForestRegressor,\n",
    "                                             base_kernel='predictions', kernel_transforms=[], **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c93c8d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ct has n_pool=41712, n_test=10700, n_features=379\n",
      "Task kegg_undir_uci has n_pool=50599, n_test=12921, n_features=27\n",
      "Running all configurations on split 0\n",
      "Results already exist for HGR_lcmd-tp_predictions on split 0 of task ct_64-128\n",
      "Results already exist for HGR_lcmd-tp_predictions on split 0 of task kegg_undir_uci_64-128\n",
      "Results already exist for CAT_lcmd-tp_predictions on split 0 of task ct_64-128\n",
      "Results already exist for CAT_lcmd-tp_predictions on split 0 of task kegg_undir_uci_64-128\n",
      "Results already exist for RF_lcmd-tp_predictions on split 0 of task ct_64-128\n",
      "Results already exist for RF_lcmd-tp_predictions on split 0 of task kegg_undir_uci_64-128\n",
      "Running all configurations on split 1\n",
      "Results already exist for HGR_lcmd-tp_predictions on split 1 of task ct_64-128\n",
      "Results already exist for HGR_lcmd-tp_predictions on split 1 of task kegg_undir_uci_64-128\n",
      "Results already exist for CAT_lcmd-tp_predictions on split 1 of task kegg_undir_uci_64-128\n",
      "Results already exist for RF_lcmd-tp_predictions on split 1 of task ct_64-128\n",
      "Results already exist for RF_lcmd-tp_predictions on split 1 of task kegg_undir_uci_64-128\n",
      "Start time: 2023-01-26 00:26:59\n",
      "Starting job 1/1 after 1s\n",
      "Running CAT_lcmd-tp_predictions on split 1 of task ct_64-128\n",
      "Test results: MAE=0.496506, RMSE=0.68247, MAXE=2.18967, q95=1.51957, q99=1.76883\n",
      "\n",
      "\n",
      "Performing AL step 1/2 with n_train=64, n_pool=41712, al_batch_size=64\n",
      "Test results: MAE=0.294175, RMSE=0.442849, MAXE=2.30026, q95=1.00094, q99=1.62777\n",
      "\n",
      "\n",
      "Performing AL step 2/2 with n_train=128, n_pool=41648, al_batch_size=128\n",
      "Test results: MAE=0.253398, RMSE=0.400413, MAXE=2.31169, q95=0.898472, q99=1.53862\n",
      "\n",
      "\n",
      "Finished running CAT_lcmd-tp_predictions on split 1 of task ct_64-128 on device cuda:0\n",
      "End time: 2023-01-26 00:27:44\n",
      "Total time: 44s\n"
     ]
    }
   ],
   "source": [
    "from bmdal_reg.run_experiments import run_experiments\n",
    "from bmdal_reg.train import ModelTrainer\n",
    "run_experiments(exp_name='relu_small', n_splits=2, run_config_list=run_configs, \n",
    "                batch_sizes_configs=[[64, 128]], task_descs=['64-128'], use_pool_for_normalization=True,\n",
    "                max_jobs_per_device=4, n_train_initial=64, ds_names=['ct', 'kegg_undir_uci'], sequential_split=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ae1f6",
   "metadata": {},
   "source": [
    "Here, the meaning of the parameters is as follows:\n",
    "- `exp_name` is the name of the subfolder that the results will be saved at. This can be used to group experiments together, for example we used separate groups for relu and silu experiments in our paper.\n",
    "- `n_splits` is the number of random splits that the configurations should be run on. The random splits will be run in order. \n",
    "- `run_config_list` is the list of run configurations created previously.\n",
    "- `batch_sizes_configs` is a list of lists of batch sizes. In our case, we only have one batch size configuration, which is to acquire 64 samples in the first BMAL step and 128 samples in the second BMAL step. For experiments in our paper, we mostly used `batch_sizes_configs=[[256]*16]`.\n",
    "- `task_descs` is a corresponding list of suffixes for the task names. For example, here the data set `ct` combined with the batch size configuration `[64, 128]` will get the name `ct_64-128`. \n",
    "- `use_pool_for_normalization` specifies whether the dataset standardization should use statistics from the training and pool set or only from the training set. We used standardization only from the training set in our experiments, but especially for smaller initial training set sizes, it may be important to standardize also with the pool set.\n",
    "- `max_jobs_per_device` allows to specify a maximum number of jobs that are run in parallel on a single device (CPU or GPU). Fewer jobs may be executed in parallel if their estimated RAM usage (see above) would otherwise exceed the remaining RAM capacity (measured at the start of `run_experiments`).\n",
    "- `n_train_initial` specifies the initial training set size, which was 256 in our experiments.\n",
    "- `ds_names` specifies the names of the data sets that experiments should be run on. Possible names can be found in the data folder specified in `custom_paths.py`. By default, all 15 data sets from the benchmark are used.\n",
    "- `sequential_split` specifies the index of the random split for which `max_jobs_per_device=1` is used; the results from this split can then be used for runtime evaluation. By default, this is set to 9. Since we only use `n_splits=2` here, this case is not reached.\n",
    "\n",
    "Since the experiments above were run on a CPU, they took 5 minutes to complete, but this would be much faster on a GPU, especially with even higher `max_jobs_per_device`. If we ran this code again, it would notice that the results are already computed and would not recompute them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019516f",
   "metadata": {},
   "source": [
    "Next, we want to evaluate the results. Unfortunately, we cannot directly use `run_evaluation.py` since its current implementation filters results by the suffix `256x16`, while our results use the suffix `64-128`. Therefore, we give a small example showing how to print a table for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "46b149ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged results across tasks:\n",
      "Results for metric mae:\n",
      "CAT_lcmd-tp_predictions: -1.189 +- 0.016\n",
      "RF_lcmd-tp_predictions:  -1.070 +- 0.019\n",
      "NN_lcmd-tp_grad_rp-512:  -1.070 +- 0.019\n",
      "HGR_lcmd-tp_predictions: -0.968 +- 0.029\n",
      "\n",
      "Results for metric rmse:\n",
      "CAT_lcmd-tp_predictions: -0.557 +- 0.018\n",
      "RF_lcmd-tp_predictions:  -0.548 +- 0.020\n",
      "NN_lcmd-tp_grad_rp-512:  -0.548 +- 0.020\n",
      "HGR_lcmd-tp_predictions: -0.506 +- 0.011\n",
      "\n",
      "Results for metric q95:\n",
      "RF_lcmd-tp_predictions:  0.205 +- 0.028\n",
      "NN_lcmd-tp_grad_rp-512:  0.205 +- 0.028\n",
      "CAT_lcmd-tp_predictions: 0.217 +- 0.001\n",
      "HGR_lcmd-tp_predictions: 0.269 +- 0.025\n",
      "\n",
      "Results for metric q99:\n",
      "HGR_lcmd-tp_predictions: 0.730 +- 0.029\n",
      "RF_lcmd-tp_predictions:  0.761 +- 0.023\n",
      "NN_lcmd-tp_grad_rp-512:  0.761 +- 0.023\n",
      "CAT_lcmd-tp_predictions: 0.772 +- 0.049\n",
      "\n",
      "Results for metric maxe:\n",
      "RF_lcmd-tp_predictions:  1.371 +- 0.018\n",
      "NN_lcmd-tp_grad_rp-512:  1.371 +- 0.018\n",
      "HGR_lcmd-tp_predictions: 1.430 +- 0.019\n",
      "CAT_lcmd-tp_predictions: 1.582 +- 0.016\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bmdal_reg.evaluation.analysis import ExperimentResults, print_avg_results\n",
    "results = ExperimentResults.load('relu_small')\n",
    "print_avg_results(results, relative_to=None, filter_suffix='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc7e2c",
   "metadata": {},
   "source": [
    "We can also print results on individual data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682bd4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task ct_64-128:\n",
      "Results for metric mae:\n",
      "NN_lcmd-tp_grad_rp-512: -1.573 +- 0.016\n",
      "NN_random:              -1.551 +- 0.010\n",
      "\n",
      "Results for metric rmse:\n",
      "NN_lcmd-tp_grad_rp-512: -1.176 +- 0.010\n",
      "NN_random:              -1.035 +- 0.011\n",
      "\n",
      "Results for metric q95:\n",
      "NN_lcmd-tp_grad_rp-512: -0.461 +- 0.012\n",
      "NN_random:              -0.257 +- 0.011\n",
      "\n",
      "Results for metric q99:\n",
      "NN_lcmd-tp_grad_rp-512: 0.139 +- 0.003\n",
      "NN_random:              0.396 +- 0.015\n",
      "\n",
      "Results for metric maxe:\n",
      "NN_lcmd-tp_grad_rp-512: 0.821 +- 0.039\n",
      "NN_random:              0.960 +- 0.009\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Results for task kegg_undir_uci_64-128:\n",
      "Results for metric mae:\n",
      "NN_lcmd-tp_grad_rp-512: -1.154 +- 0.201\n",
      "NN_random:              -0.961 +- 0.057\n",
      "\n",
      "Results for metric rmse:\n",
      "NN_lcmd-tp_grad_rp-512: -0.424 +- 0.136\n",
      "NN_random:              -0.161 +- 0.035\n",
      "\n",
      "Results for metric q95:\n",
      "NN_lcmd-tp_grad_rp-512: 0.310 +- 0.109\n",
      "NN_random:              0.435 +- 0.030\n",
      "\n",
      "Results for metric q99:\n",
      "NN_lcmd-tp_grad_rp-512: 1.016 +- 0.117\n",
      "NN_random:              1.280 +- 0.092\n",
      "\n",
      "Results for metric maxe:\n",
      "NN_lcmd-tp_grad_rp-512: 1.995 +- 0.185\n",
      "NN_random:              2.373 +- 0.011\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation.analysis import print_all_task_results\n",
    "print_all_task_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a25803",
   "metadata": {},
   "source": [
    "The results are saved using the folder structure `results_folder/exp_name/task_name/alg_name/split_idx/results.json`. For example, you can view the tasks we ran experiments on as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fba3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ct_64-128', 'kegg_undir_uci_64-128']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import custom_paths\n",
    "os.listdir(Path(custom_paths.get_results_path()) / 'relu_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41904c0a",
   "metadata": {},
   "source": [
    "## Implementing your own methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971d246",
   "metadata": {},
   "source": [
    "If you want to go beyond using our already implemented combinations of selection methods, kernels and kernel transformations, you have to make some modifications to the code such that your method can be used as above. Depending on how different your method is, we suggest three ways of including it:\n",
    "- If your method fits to our framework and simply provides new selection methods, kernels and/or kernel transformations, we suggest to extend `BatchSelector.select()` in `bmdal/algorithms.py` such that it can use your new component(s) given the corresponding configuration string(s).\n",
    "- If your method is a different BMDAL method that does not fit into our framework but does not require to modify the NN training process, you can modify the BMDAL part in `ModelTrainer.__call__()` in `train.py` such that it can call your method. While you could realize this by passing a custom BMDAL class or factory method directly to ModelTrainer, you should note that arguments to ModelTrainer are currently serialized to a JSON file, which is why we prefer using native data types like strings as arguments to ModelTrainer. This serialization of arguments to a JSON file can be helpful for example for automatically generating figure captions later on.\n",
    "- If you also want to modify the NN training process, you may want to change other code in ModelTrainer or replace it by your own custom class. Note that the NN model creation itself can be customized in ModelTrainer through the `create_model` argument."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
